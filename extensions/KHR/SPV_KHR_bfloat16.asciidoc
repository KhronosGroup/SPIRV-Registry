SPV_KHR_bfloat16
================

Name Strings
------------

SPV_KHR_bfloat16

Contact
-------

To report problems with this extension, please open a new issue at:

https://github.com/KhronosGroup/SPIRV-Registry

Contributors
------------

- Tobias Hector, AMD
- Stu Smith, AMD
- Jeff Bolz, Nvidia
- KÃ©vin Petit, Arm
- David Neto, Google
- Graeme Leese, Broadcom
- Ruihao Zhang, Qualcomm
- Mark Sheppard, Imagination
- Ben Ashbaugh, Intel
- Dmitry Sidorov, Intel
- Victor Mustya, Intel
- Alan Baker, Google

Notice
------

Copyright (c) 2023-2025 The Khronos Group Inc. Copyright terms at
http://www.khronos.org/registry/speccopyright.html

Status
------

- Approved by the SPIR-V Working Group: 2025-01-29
- Approved by the Khronos Board of Promoters: 2025-03-14

Version
-------

[width="40%",cols="25,25"]
|========================================
| Last Modified Date | 2025-01-29
| Revision           | 1
|========================================

Dependencies
------------

This extension is written against the SPIR-V Specification,
Version 1.6 Revision 4.

This extension requires SPIR-V 1.0.

This extension interacts with SPV_KHR_cooperative_matrix.


Overview
--------

This extension extends the *OpTypeFloat* instruction to enable the use of bfloat16 types with cooperative matrices and dot products, and enabling conversions between other types.


Extension Name
--------------

To use this extension within a SPIR-V module, the following
*OpExtension* must be present in the module:

----
OpExtension "SPV_KHR_bfloat16"
----


Modifications to the SPIR-V Specification, Version 1.6
------------------------------------------------------

FP Encoding
~~~~~~~~~~~

Add a new enum:

--
[cols="^2,14,2,4",options="header",width = "100%"]
|====
2+^.^| FP Encoding | Width(s) | Enabling Capabilities

| 0   | *BFloat16KHR* +
The floating point type is encoded as the bfloat16 type, which is equivalent to the IEEE754 binary32 floating point encoding but with 16 fewer bits in its significand.
This is encoded per chapter 3 of the IEEE 754-2008 standard with the following encoding parameters:

 - _bias_ is 127
 +
 - _sign bit_ is 1
 +
 - _w_ (exponent) is 8
 +
 - _t_ (significand) is 7
 +
 - _k_ (width) is 16

The additional precision parameters _p_ and _emax_ are calculated as __p = t+1__ and __emax = bias__.
The radix (or base) _b_ is 2. +
 +
The accuracy of floating-point operations on *BFloat16KHR* values is the same as operations on IEEE754 binary32 floating-point values, with the result rounded using round-to-nearest-even or round-to-zero rounding.
It is implementation-defined whether intermediate values are rounded.
| 16 | *BFloat16TypeKHR*
|====
--

Type-Declaration Instructions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Add the following requirement to *OpTypeCooperativeMatrixKHR*:

    If Component Type has a *BFloat16KHR* encoding then *BFloat16CooperativeMatrixKHR* must be declared.

Conversion Instructions
~~~~~~~~~~~~~~~~~~~~~~~

Add the following paragraph to *OpFConvert*, *OpConvertFToU*, *OpConvertFToS*, *OpConvertSToF*, and *OpConvertUToF*:

    Conversions to or from floating-point values with the `BFloat16KHR` encoding first convert the source value to IEEE754 binary32, and then from IEEE754 binary32 to the target format.
    When converting from `BFloat16KHR` to IEEE754 binary32, the additional significand bits are padded with 0s, producing the exact same value.
    When converting from IEEE754 binary32 to `BFloat16KHR`, the rounding mode is defined by the client API.


Arithmetic Instructions
~~~~~~~~~~~~~~~~~~~~~~~

Add the following requirement to *OpDot*:

    If the Result Type (and the component types of Vector 1 and Vector 2) has a *BFloat16KHR* encoding then *BFloat16DotProductKHR* must be declared.


Capabilities
~~~~~~~~~~~~

Modify Section 3.31, Capability, adding this row to the Capability table:

--
[options="header",cols="1,5,2"]
|====
2+^| Capability ^| Implicitly Declares
| 5116 | *BFloat16TypeKHR* +
Uses *OpTypeFloat* to specify types with the *BFloat16KHR* <<Floating_Point_Encoding, floating point encoding>> and the use of conversion functions for those types.
|
| 5117 | *BFloat16DotProductKHR* +
Uses vectors with a *Component Type* of *OpTypeFloat* with the *BFloat16KHR* encoding with *OpDot*.
| *BFloat16TypeKHR*
| 5118 | *BFloat16CooperativeMatrixKHR* +
Uses cooperative matrices with a *Component Type* of *OpTypeFloat* with the *BFloat16KHR* encoding.
| *BFloat16TypeKHR*, *CooperativeMatrixKHR*
|====
--


Validation Rules
~~~~~~~~~~~~~~~~

Add the following bullets to section 2.16.1, Universal Validation Rules:

  * Objects with a type that is or includes a floating-point type with the *BFloat16KHR* encoding must only be used with the following instructions:
  ** https://registry.khronos.org/SPIR-V/specs/unified1/SPIRV.html#_constant_creation_instructions[Constant Creation Instructions]
  ** https://registry.khronos.org/SPIR-V/specs/unified1/SPIRV.html#_memory_instructions[Memory Instructions]
  ** https://registry.khronos.org/SPIR-V/specs/unified1/SPIRV.html#_function_instructions[Function Instructions]
  ** https://registry.khronos.org/SPIR-V/specs/unified1/SPIRV.html#_conversion_instructions[Conversion Instructions]
  ** https://registry.khronos.org/SPIR-V/specs/unified1/SPIRV.html#_composite_instructions[Composite Instructions]
  ** OpDot
  ** OpCooperativeMatrixMulAddKHR
  ** OpCooperativeMatrixLengthKHR
  ** OpSelect
  ** OpPhi
  ** OpReturnValue
  ** OpLifetimeStart
  ** OpLifetimeStop


Issues
------

What is the origin of the naming for bfloat16?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The type was originally named "brain float" as it was developed by Google Brain, an artificial intelligence group at Google, and it is a 16-bit type.
The name is generally shortened to bfloat16.


Revision History
----------------

[cols="5,15,15,70"]
[grid="rows"]
[options="header"]
|==============================================
| Rev |    Date    | Author  | Changes
|  1  | 2025-01-29 | TobiasH | Initial revision
|==============================================